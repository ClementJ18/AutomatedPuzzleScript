\chapter{Introduction}
\label{ch:introduction}
% Context: what is the bigger scope of the problem you are trying to solve? Try to connect to societal/economical challenges.
% Problem Analysis: Here you present your analysis of the problem situation that your research will address.
% How does this problem manifest itself at your host organisation?
% Also summarises existing scientific insight into the problem.

Digital gaming is a considerable market, making up 2.7 billion users, worth over \$300 billion according to a report by Accenture\footnote{\url{https://newsroom.accenture.com/news/global-gaming-industry-value-now-exceeds-300-billion-new-accenture-report-finds.htm}}. Game design is a vital part of the game development process that focuses on the theoretical creation of the game. Game designers aim to create games that are both interesting and interactive aimed at experiences such as entertainment or education. 

% Game design refers to the process of decision making on what a game will look, sound and play like, game development is the implementation of those decisions. 

A key part of the game design is the creation of \emph{game mechanics}, sets of rules that define how the player can interact with the game. Gameplay emerges from game mechanics\cite{adams2012game}. The quality of gameplay is defined by the interaction between game mechanics and the player. Since the quality of gameplay is directly tied with the player experience it varies from player based on their personal preferences. Validating the impact of game mechanics on a subjective player experience requires \emph{extensive playtesting}, both generally and within specific control groups. This process is costly in terms of time and money\cite{ostrowski2013automated} and is subject to human error and inconsistencies.


% If a game is considered fun or not depends on the quality of the gameplay created by the player interacting with the game mechanics. Being able to tell if a game is fun simply from studying game mechanics is difficult and designers need the mechanics to be playtested, either by themselves or by someone else in order to validate them.  

% Missed release dates have huge actual and opportunity costs for companies and can even mean bankruptcy for smaller ones. 

When game requirements inevitably change, the designers must evolve game mechanics to meet them. To analyze the impact of these changes, designers use techniques like prototyping and playtesting, but gameplay quality degradation my happen nonetheless. When designers attempt to improve a game's rules they take a risk that can end up negatively impacting their game. This problem is exacerbated by the little time designers have for improving games.

% PAPER PROTOTYPING

% , which results in certain parts of the game having their difficulty trivialised or on the contrary, increased to the point where it makes it impossible to progress further. 

% Objectives.
% We aim to empower designers with languages and tools that automate the game design process and enable improving games more quickly.
% To create these tools, we explore the relationship between rules and gameplay by studying PuzzleScript.
% PuzzleScript is a domain-specific language and engine with a thriving community that has created many puzzle games whose source code we can analyse to answer our research questions.
% We pose the question: How can languages, techniques and tools help designers to predict the effects of iterative design changes they have to make to the source code of a puzzle’s rules to improve the gameplay?
%  (Note: Too long, hard to read, perhaps break into parts / subquestions)

Our aim is to empower designers with tools that automate the game design process and enable them to evolve games quickly. To create these tools, we explore the relationship between rules and gameplay by studying PuzzleScript. PuzzleScript is a popular open-source tiny online game engine\cite{8901975} written in JavaScript by Stephen Lavelle. We analyze the source code of many games created with PuzzleScript to answer our research questions. We explore how languages, tools, and techniques can help designers understand the impact of the changes they make in order to improve gameplay. We study the effect of iterative changes to the source code on the game mechanics.


% Approach.
% We automate the game design process by

% We combine theory from two key areas: automated game design \cite{few people, maybe my mapping study} and software evolution \cite{Mens?}.

% We reverse engineer PuzzleScript
% We apply Rascal, a meta-programming language and language work bench.

We automate the game design process by providing rapid live feedback on the state of the game mechanics and the impact of changes. We combine theories from two key areas: automated game design\cite{ostrowski2013automated,cook2020software,cook2017vision,10.1145/3102071.3102084,10.1145/3412843} and software evolution\cite{10.1145/2501543.2501546,1572302}. PuzzleScript's official implementation is aimed at the portability of games and performance. However, these advantages come at the cost of reduced maintainability and extensibility, this makes it complicated to perform automated dynamic analysis on the code. We reverse engineer PuzzleScript in order to create a formal design document that will help us better understand how we can support developers that use the language. We then re-implement that design and optimize it for our research purposes using Rascal, a meta-programming language and language workbench. Finally, we extend our prototype implementation to perform dynamic analysis on PuzzleScript games and allow a better understanding of the impact of game evolution.


% Finally, we extend our prototype PuzzleScript implementation allowing game designers to perform dynamic analysis on their PuzzleScript games and the impact that rules have on the gameplay. 

\section{Problem statement}
%clarify why we really need validation
We now elaborate on the research problem. 

Playtesting has a large impact on the quality of the gameplay. Game mechanics that are thoroughly playtested are less likely to negatively impact the quality of the gameplay. Since game mechanics indirectly affect each other, this means that changes to a game mechanic can have unintended side effects when combined with others. As such, game designers must playtest not only how a mechanic can interact with the game but also how it interact with every other mechanic. With every new mechanic, the amount of playtesting required grows exponentially.

% Playtesting mechanics and their interaction with one another is an exponential task. 

The process of validating game mechanics through QA testing is resource-intensive and time-consuming. It consists of testers, either human or AI-based, playing through the game and reporting on their impressions, this feedback can then be used by game designers to further evolve the game. Because of the time required to playtest, game designers have to make choices on what part of the game they need feedback on. The priority usually goes to the new content, as such, existing content is not always regression tested when new mechanics are introduced\cite{ostrowski2013automated} 


% The priority usually goes to new content, whereas existing content that has only been modified do not always get fully regression tested\cite{ostrowski2013automated}.

The time constraint exists in both large companies that have financial goals to meet to show growth and in small companies that depend on the revenue to continue functioning. Any game has a labor cost associated with it and most game designers seek to break even. As a result, most game design projects have a sort of deadline, either 'eyeballed' or specifically tailored for the costs of the game to meet the expected revenue. This deadline is the reason that playtesting cannot always be conducted fully.  

On a smaller scale, the time constraint also restricts how much game designers can explore the design space of their game. Design space is defined by Schubert in his blog "Understanding Design Space"\footnote{\url{https://www.zenofdesign.com/understanding-design-space/}} as "he canvas that the designer can paint on". Understanding the size of that 'canvas' and its growth potential are key to understanding what a game can achieve in terms of content. Game mechanics define the size of the design space. A match-three game will have a much more limited design space than a grand strategy civilization game.

The issue of incomplete playtesting has a significant impact on the quality of games as they begin to shift to a "games as a service" (GaaS) model where game mechanics are intentionally meant to evolve after the initial release. Small digital game companies are especially affected by this as they do not always have the resources or foresight to see the importance of gameplay quality assurance measures\cite{10.1145/2994310.2994364}. More and more game designers are beginning to rely on their players to test interactions for them and release moderately tested patches that may or may not fix the issues reported\cite{Brühlmann_Schmid_Mekler_2016}. To resolve this we propose a system of solutions based on meta-programming that allow us to conduct Automated Game Design (AGD) through the automation of rule validation. We approach this goal by studying PuzzleScript and the relations between the source code of PuzzleScript, which has an explicit notation for game mechanics and how that code actually impacts gameplay.

% between the source code of PuzzleScript game and their mechanics. 


% The source code of PuzzleScript game has a very close relationship with the mechanics of that game, this allows us to test our approach on a specific problem. 


This relation between code and game mechanics allows us to test our approach on a concrete problem. However, we run into issues when trying to analyze PuzzleScript:
\begin{itemize}
    \item PuzzleScript has no formal design document. This makes it challenging to study it for ways to apply our dynamic analysis.
    \item PuzzleScript's official JavaScript implementation is focused on portability and performance. This makes it challenging to process and analyze the source code.
\end{itemize}

These two issues are significant obstacles to our project and we cannot progress without either resolving them or selecting another language. We choose to continue forward with PuzzleScript because of its popularity and the relation mentioned above. Additionally, fixing these issues should make it easier to perform research using PuzzleScript as a concrete problem by extending a general purpose tool to fit a specific problem.


% We choose to stick with PuzzleScript and resolve the issues with the aim of making research using PuzzleScript generally easier.


\subsection{Research questions}
% To tackle these issues, we investigate ...
% To tackle the problem stated above, we propose two research questions. The first question focuses on the general problem described, empowering video game developers by providing rapid feedback from a meta-programming aspect. We want to understand how much of an impact \textbf{fast} feedback will have compared to current approaches that already provide feedback but at a much slower rate. Rapid feedback will allow game designers to validate their game mechanics not just when they initially create them but also whenever they evolve them. Therefore we ask:

% \textbf{RQ1}: How can tools and techniques empower game designers and allow them to more freely explore the design space of their games?
% \begin{itemize}
%     \item \textbf{RQ1.1}: How can tools and techniques support game designers evolving their game mechanics? 
%     \item \textbf{RQ.1.2}: How can tools and techniques provide support for existing game engines?
%     \item \textbf{RQ1.3}: How can tools and techniques mitigate the negative impacts of game mechanics on player experience?
% \end{itemize}

% This general problem is then applied concretely to the case of PuzzleScript. This makes the question simpler to answer with the trade-off that proving it can be applied specifically does not fully validate our general problem. We ran into additional problems with studying PuzzleScript which become additional subquestions. Answering these subquestions will allow us to make better use of PuzzleScript as we aim to empower developers.

% \textbf{RQ2}: How can live feedback on gameplay and fun of a PuzzleScript game help game designers create better games?
% \begin{itemize}
%     \item \textbf{RQ2.1}: What are the limits of the current implementation of Puzzlescript? How does the current grammar of PuzzleScript (or lack thereof) create these limitations?
%     \item \textbf{RQ2.2}: How does a Rascal re-design of Puzzlescript deal with those limits? What new limits arise (if any)?
%     \item \textbf{RQ2.3}: How can a game analysis tool based on the PuzzleScript grammar leverage meta-programming principles?
% \end{itemize}

Our goal is to help reduce the resources required to playtest game mechanics by allowing for automated playtesting to be conducted. We hypothesize that automatically playtesting for simple errors in the game mechanics will allow game designers to focus human playtesting on complex issues. This 'automated playtesting' must be relatively fast so that game designers do not feel reluctant to run it but thorough enough to still pick up on any major flaws in the game mechanic.

We seek to create tools and techniques that will help developers automatically playtest their games not only when they are first created but also as they evolve. The impact game mechanics have on the user experience changes over time, both as a result of direct and indirect changes. As such, we aim to use of meta-programming tools and techniques to: 1) empower developers in their effort to evolve their games; 2) support popular established game engines; 3) mitigate the negative impacts of game mechanics on player experience.

% \begin{itemize}
%     \item \textbf{empower} developers in their effort to evolve their games.
%     \item \textbf{support} popular established game engines.
%     \item \textbf{mitigate} the negative impacts of game mechanics on player experience.
% \end{itemize}

The tools we provide aim to leverage meta-programming principles. However, this goal is very general and therefore hard to concretely apply. We resolve this by specifically studying PuzzleScript, which leads us to formulate the following questions:

\textbf{RQ1}: How can meta-programming tools and techniques be used to empower PuzzleScript game designers and allow them to freely explore the design space of their game?
\begin{itemize}
    \item \textbf{RQ1.1}: How can PuzzleScript be used for studying game mechanics?
    \item \textbf{RQ1.2}: How can game designers mitigate the impact of iterative game design on the quality of the game mechanics in PuzzleScript games?
\end{itemize}

One of our goals in this project is to not only test our theory of simple automated mechanics testing but also to provide software contributions to the field of automated game research in PuzzleScript. We aim to make it easier to conduct research into PuzzleScript by creating general-use tools that can be extended to fit individual projects.

% Answering this question should allow us to create knowledge that we can then apply to our goal. It should also address the issues we raised in the previous section concerning technical obstacles to studying PuzzleScript. 

\subsection{Research method}
We conduct design science research\cite{dr2}. This means that we go through an iterative and cyclic process composed of three steps. We gather problems from our environment, propose solutions supported by knowledge bases, and then validate them with field tests to see if they match our requirements. This is a mixed method that is combined with action research\cite{dr1}. Our action in this case is the tool we propose and we analyze the impact it has on the environment.

We study the existing PuzzleScript's design to understand how code relates to gameplay. We survey common discussion areas to understand what PuzzleScript game designers' needs. We then implement this into our tool to conduct an analysis of the game and provide feedback based on that. We validate our tool by testing it on published games and mimicking software evolution. We can then observe what kind of feedback the tool provides.

% We validate our tool by creating bad games and seeing what kind of feedback our tool provides.

% When researching we wanted to insure that we kept our work relevant, hence we apply the cycle of relevancy to help

\section{Contributions}
% \begin{figure}[!t]
%     \centering
%     \begin{tabular}{lll}
%         RQ  & Chapter                          & Contribution                          \\
%         % 1   & 7 Conclusion                     & Gameplay Analysis                     \\
%         % 1.1 & 2 Background                     & Absence of Fun as a Quality Criteria  \\
%         % 1.2 & 5 Discussion                     & Tool applied on Case Study            \\
%         1   & 5 Discussion                     & Functionality Validation              \\
%         1.1 & 3.2 Redesigning PuzzleScript     & PuzzleScript implementation in Rascal \\
%         1.2 & 3.3 Extending our implementation & Dynamic Rule Analysis and IDE Support \\
%     \end{tabular}
%     \caption{Software contributions}
%     \label{fig:contributions}
% \end{figure}

% In this work, we contribute a tool that can be used to run and analyse PuzzleScript games. Creating this tool has led us to formalize PuzzleScript's design to better our understanding of this powerful engine. To be able to analyse PuzzleScript code we also created our own implementation based on a redesign of the JavaScript implementation. Finally, to implement our Rule Analysis we extend our implementation and add IDE support such as syntax highlighting. The result is a tool that can be more easily extended and maintained than the current implementation of PuzzleScript be it either game designers seeking to add additional features or researchers who desire specific stats. These contributions are used to answer \textbf{RQ2}. We then contribute our answers to our goal by extrapolating general knowledge from the specific knowledge created. 

The main contribution of this thesis is a prototype tool called ScriptButler, for evolving, analyzing, and running PuzzleScript games. ScriptButler is not intended as a be-all/end-all solution but rather a general-purpose tool that can be extended to meet project-specific needs. At its base, the tool offers an interface to parse, process, and statically analyze PuzzleScript game with the possibility for extension based on the user needs. This can be implementing changes to PuzzleScript design or simply adding new restrictions to enhance the live feedback of a specific project. The more advanced part of the tool provides an engine through which the game is compiled and can then be run. However, Rascal does not have a method to gather user input as such the tool provides an interface for interacting with the game, but a user may also choose to implement their own. An earlier version of this work was previously published as \fullcite{Rozen2021a}.

The tool also provides dynamic analysis for PuzzleScript games. The games are compiled and then run by the engine in the background to generate helpful information on how the various mechanics interact. This dynamic analysis also uses a system of simple solutions to rapidly test out any potential issues with the game. Finally, the tool provides IDE integration within Eclipse for PuzzleScript games. This integration has syntax coloring, shows messages by the checkers, and provides other utilities such as an outline and code folding. We also contribute knowledge to the Automated Game Design area through our theory of rapid feedback allowing for better quality of games. A theory that we test using our tool on PuzzleScript games.

% We also contribute knowledge on gameplay analysis and determining the quality of game mechanics through the system of solutions we propose. This system of solutions aims to allow for rapid analysis of game mechanics by focusing on whether or not games show any obvious bad design pattern rather than focusing on whether a game follow good game design which is much more subjective. This knowledge we create, and validate with our tool, is then used to answer \textbf{RQ1}.

% We can then apply our tools to PuzzleScript games to answer \textbf{RQ1}. We extrapolate answers from our specific case of studying PuzzleScript into more general knowledge. Each of the subquestions from \textbf{RQ2} is used to answer a subquestion from \textbf{RQ1}. With an example of each subquestion answered we then aim to provide an answer for \textbf{RQ1} to apply the knowledge created in a general way. 


\section{Outline}
% In Chapter~\ref{ch:background} we describe the background of this thesis. 
% Chapter~\ref{ch:research} describes ... 
% Results are shown in Chapter~\ref{ch:results} and discussed in Chapter~\ref{ch:discussion}. Chapter~\ref{ch:related_work}, contains the work related to this thesis.
% Finally, we present our concluding remarks in Chapter~\ref{ch:conclusion} together with future work.

In Chapter \ref{ch:background}, we elaborate on the research that supports our thesis and go into the related work that we used as a basis. This is different from the Chapter \ref{ch:related_work} where we discuss works that relate to our work but that are not necessarily used directly to support our work. Rather, that chapter is intended to display what kind of research is being done into PuzzleScript and Automated Game Design.

In Chapter \ref{ch:puzzlescript}, we present the design document we obtained by reverse-engineering PuzzleScript's design. We briefly discuss the design of PuzzleScript and its official JavaScript implementation.

In Chapter \ref{ch:research}, we explain the design of our tool and the work that led up to those decisions based on the document presented last chapter. This chapter consists of two sections: re-designing the technical implementation of PuzzleScript and extending our implementation with dynamic analysis to support game designers.

In Chapter \ref{ch:case}, we discuss the case study we conduct to validate our tool. We layout the game we choose, with the reason we choose it. Then we go into the method we use for conducting the case study. Finally, we discuss the results of our case studies and whether they successfully validate our tool. 

Chapter \ref{ch:discussion}, we reflect on the main results of this thesis. We discuss how our work answers our research questions. We also discuss the threats to validity and how we mitigated them.

Finally, in Chapter \ref{ch:conclusion}, we summarize our contributions, give a few concluding remarks, and discuss future work.

